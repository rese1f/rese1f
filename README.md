# Hi, I'm Wenhao Chai üëã

I'm a graduate student at **University of Washington** focused on computer vision research in large multimodal models, video understanding, embodied agents, and generative models. I love exploring innovative ideas that bridge theory and real-world applications. I believe computer vision will continue to advance.

## About Me

- üéì **Graduate Student** at University of Washington
- üìö Former undergraduate at Zhejiang University  
- üî¨ Research interests: video understanding, LMM, and generative models
- ü§ù Collaborated with leading labs and researchers including the [Pika Labs](https://pika.art/), [Microsoft Research Asia](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/), [Information Processing Lab @ UW](https://ipl-uw.github.io), [CVNext Lab @ ZJU](https://cvnext.github.io), and more.

**Docs & Profiles:**  
- [Website](https://rese1f.github.io)
- [CV](https://rese1f.github.io/assets/file/cv.pdf)  
- [Google Scholar](https://scholar.google.com/citations?user=SL--7UMAAAAJ&hl=en)  

## Pinned Message

- To junior master/undergraduate students: if you would like to chat about life, career plan, or research ideas related to AI/ML, feel free to send me zoom / google meet invitation via email to schedule a meeting. I will dedicate at least 30 mins every week for such meetings. I encourage students from underrepresented groups to reach out.
- We are hosting Discord server among professors and students for arXiv daily sharing and research discussion.

## Research & Projects

My current research currently focus on developing <b>visual intelligence</b> to understand the physical world, building upon video understanding as a core perceptual mechanism. I propose a long-short term memory framework modeled after the human memory system, enabling pre-trained video Large Multi-modal Models (LMMs) to comprehend hour-long video content without additional fine-tuning. To enhance efficiency, I introduce token merging to LMMs, significantly reducing visual tokens with minimal performance degradation. I also demonstrate step-by-step agent system development in Minecraft, showcasing cognitive-inspired agent capabilities in virtual environments.

I have just begun shaping my research narrative around visual intelligence. With "pretraining as we know it will end"</i>, I believe the future of artificial intelligence lies in aligning with human intelligence‚Äîand ultimately surpassing it. I truly believe computer vision will continue to advance.

Here are some of my recent key projects and publications:

- **(ICLR 2025) AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark**  
  *Efficient and performant video detailed captioning, plus a new benchmark.*  
  [Project Page](https://rese1f.github.io/aurora-web/) | [Paper](https://arxiv.org/abs/2410.03051) | [Code](https://github.com/rese1f/aurora)
 
- **(CVPR 2024) MovieChat: From Dense Token to Sparse Memory in Long Video Understanding**  
  *From dense token to sparse memory: advancing long video understanding with state-of-the-art memory mechanisms.*  
  [Project Page](https://rese1f.github.io/MovieChat/) | [Paper](https://arxiv.org/abs/2307.16449) | [Code](https://github.com/rese1f/MovieChat)

- **(ICCV 2023) StableVideo: Text-driven Consistency-aware Diffusion Video Editing**  
  *Text-driven, consistency-aware diffusion video editing for generating coherent video content.*  
  [Project Page](https://rese1f.github.io/StableVideo/) | [Paper](https://arxiv.org/abs/2308.09592) | [Code](https://github.com/rese1f/StableVideo)

_For a full list of my publications and detailed project descriptions, please visit my [website](wenhaochai.com)._

## Connect with Me

- üìß **Email:** [wchai@uw.edu](mailto:wchai@uw.edu)
- üê¶ **Twitter:** [@wenhaocha1](https://x.com/wenhaocha1)
- üíº **LinkedIn:** [Wenhao Chai](https://www.linkedin.com/in/wenhao-chai-658274238/)
- ü§ó **Hugging Face:** [Profile](https://huggingface.co/wchai)

---

*This README is inspired by my personal website ‚Äì [wenhaochai.com](wenhaochai.com). Feel free to explore for more details about my research and projects.*

<picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/rese1f/rese1f/output/github-contribution-grid-snake-dark.svg">
  <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/rese1f/rese1f/output/github-contribution-grid-snake.svg">
  <img alt="github contribution grid snake animation" src="https://raw.githubusercontent.com/rese1f/rese1f/output/github-contribution-grid-snake.svg">
</picture>

