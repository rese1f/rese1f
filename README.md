On 23 FA, I will be a master student of Information Processing Lab at University of Washington, advised by Professor Jenq-Neng Hwang. I am currently working on embodied agent and video understanding.

When I am not doing research, I like photography, traveling, and singing.

<div align="center">
<a href="https://rese1f.github.io/" target="_blank">
<img src=https://img.shields.io/badge/home-%239cf.svg?&style=for-the-badge&logo=github&logoColor=white alt=website style="margin-bottom: 5px;" />
</a>
<a href="https://github.com/rese1f" target="_blank">
<img src=https://img.shields.io/badge/github-%2324292e.svg?&style=for-the-badge&logo=github&logoColor=white alt=github style="margin-bottom: 5px;" />
</a>
<a href="https://linkedin.com/in/wenhao-chai-658274238/" target="_blank">
<img src=https://img.shields.io/badge/linkedin-%231E77B5.svg?&style=for-the-badge&logo=linkedin&logoColor=white alt=linkedin style="margin-bottom: 5px;" />
</a>
<a href="https://twitter.com/re5e1f" target="_blank">
<img src=https://img.shields.io/badge/twitter-%232E87FB.svg?&style=for-the-badge&logo=twitter&logoColor=white alt=twitter style="margin-bottom: 5px;" />
 <a href="./src/wechat.jpg" target="_blank">
<img src=https://img.shields.io/badge/wechat-%a3c62b.svg?&style=for-the-badge&logo=wechat&logoColor=white alt=wechat style="margin-bottom: 5px;" />
</a>  
<a href="https://www.instagram.com/rese1f/" target="_blank">
<img src=https://img.shields.io/badge/instagram-e1306c.svg?&style=for-the-badge&logo=instagram&logoColor=white alt=instagram style="margin-bottom: 5px;" />
</a>  
</div>

<br>

<hr style="height:2px;border-width:0;color:gray;background-color:gray">
<b><i class="fa-solid fa-pen-to-square" style="font-size:24px"></i> My GPTs:</b><br><br>

- [Academic Paper Writing Assistant](https://chat.openai.com/g/g-3JjMSVsuP-academic-paper-writing-assistant): For AI academic papers.
- [Paper Search Engine](https://chat.openai.com/g/g-9v5gHG9Bo-paper-search-engine): Expert in latest academic paper search and summary.

<hr style="height:2px;border-width:0;color:gray;background-color:gray">
<b><i class="fa-solid fa-pen-to-square" style="font-size:24px"></i> Updates:</b><br><br>

<ul>

<li><i>Oct 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Efficient Domain Adaptation via Generative Prior for 3D Infant Pose Estimation</i> is accepted by WACV 2024 workshop: <a href="https://cv4smalls.sites.northeastern.edu/">CV4Smalls</a>.
	</li><br>

<li><i>Oct 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation</i> is accepted by WACV 2024.
	</li><br>

<li><i>Sept 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Devil in the Number: Towards Robust Multi-modality Data Filter</i> is accepted by ICCV 2023 workshop: <a href="https://www.datacomp.ai/">TNGCV-DataComp</a>.
	</li><br>

<li><i>Sept 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models</i> is accepted by IEEE T-MM.
	</li><br>
 
<li><i>Aug 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Five A+ Network: You Only Need 9K Parameters for Underwater Image Enhancement</i> is accepted by BMVC 2023.
	</li><br>
 
<li><i>July 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Sequential Affinity Learning for Video Restoration</i> is accepted by ACM MM 2023.
	</li><br>

<li><i>July 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Enhanced 3D Human Pose Estimation</i> is accepted by ACM MM 2023.
	</li><br>

<li><i>July 2023:</i> <i class="fa-regular fa-copy" style="font-size:20px"></i> Our project <i>MovieChat: From Dense Token to Sparse Memory in Long Video Understanding</i> is released at <a href="https://rese1f.github.io/MovieChat/">website</a>.
	</li><br>

<li><i>July 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>StableVideo: Text-driven Consistency-aware Diffusion Video Editing</i> is accepted by ICCV 2023.
	</li><br>

<li><i>July 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation</i> is accepted by ICCV 2023.
	</li><br>

<li><i>Apr 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> The short version of our paper <i>DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models</i> is accepted by CVPR 2023 <a href="http://conferences.visionbib.com/2023/cvpr-cvfad-6-23-call.html">6th Workshop on Computer Vision for Fashion, Art, and Design</a>.
	</li><br>

<li><i>Mar 2023:</i> <i class="fa-regular fa-note-sticky" style="font-size:20px"></i> Our paper <i>Deep Learning Methods for Small Molecule Drug Discovery: A Survey</i> is accepted by IEEE T-AI.
	</li><br>

</ul>
